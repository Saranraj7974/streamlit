{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IokE2edSlLMO"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "nifty50_tickers = [\n",
        "    \"RELIANCE.NS\", \"TCS.NS\", \"INFY.NS\", \"HDFCBANK.NS\", \"ICICIBANK.NS\",\n",
        "    \"HINDUNILVR.NS\", \"ITC.NS\", \"LT.NS\", \"SBIN.NS\", \"KOTAKBANK.NS\",\n",
        "    \"AXISBANK.NS\", \"ASIANPAINT.NS\", \"BAJFINANCE.NS\", \"BHARTIARTL.NS\",\n",
        "    \"HCLTECH.NS\", \"SUNPHARMA.NS\", \"MARUTI.NS\", \"NTPC.NS\", \"TITAN.NS\",\n",
        "    \"ULTRACEMCO.NS\", \"WIPRO.NS\", \"TECHM.NS\", \"POWERGRID.NS\", \"NESTLEIND.NS\",\n",
        "    \"JSWSTEEL.NS\", \"TATASTEEL.NS\", \"ADANIENT.NS\", \"COALINDIA.NS\", \"BAJAJFINSV.NS\",\n",
        "    \"DIVISLAB.NS\", \"GRASIM.NS\", \"HDFCLIFE.NS\", \"CIPLA.NS\", \"SBILIFE.NS\",\n",
        "    \"BRITANNIA.NS\", \"EICHERMOT.NS\", \"HEROMOTOCO.NS\", \"BAJAJ-AUTO.NS\", \"INDUSINDBK.NS\",\n",
        "    \"BPCL.NS\", \"SHREECEM.NS\", \"DRREDDY.NS\", \"HINDALCO.NS\", \"ONGC.NS\",\n",
        "    \"TATAMOTORS.NS\", \"APOLLOHOSP.NS\", \"M&M.NS\", \"UPL.NS\", \"ADANIPORTS.NS\"\n",
        "]\n",
        "\n",
        "data = yf.download(nifty50_tickers, start=\"2020-01-01\", end=\"2025-01-01\")\n",
        "data.to_csv(\"nifty50_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove NaN values\n",
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "yG4vbSrRlTAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ks9L0N5tlchH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show cleaned data\n",
        "\n",
        "print(\"Cleaned Data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Calculate daily returns\n",
        "returns = data['Close'].pct_change().dropna()\n",
        "\n",
        "# Calculate log returns\n",
        "log_returns = np.log(1 + returns).dropna()\n",
        "\n",
        "print(\"\\nDaily Returns:\")\n",
        "print(returns.head())\n",
        "print(\"\\nLog Returns:\")\n",
        "print(log_returns.head())"
      ],
      "metadata": {
        "id": "UgFRgSStlllG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Summary statistics\n",
        "print(returns.describe())\n",
        "\n",
        "# Heatmap of correlations\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(returns.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix of Nifty 50 Stocks\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WQgNIyWYlozl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Risk return summary\n",
        "\n",
        "risk_return_summary = pd.DataFrame({\n",
        "    'Mean Daily Return': returns.mean(),\n",
        "    'Daily Volatility (Std Dev)': returns.std()\n",
        "}).sort_values(by='Mean Daily Return', ascending=False)\n",
        "\n",
        "print(\"\\nRisk-Return Summary:\")\n",
        "print(risk_return_summary)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Daily Volatility (Std Dev)', y='Mean Daily Return', data=risk_return_summary, s=100)\n",
        "for ticker in risk_return_summary.index:\n",
        "    plt.text(risk_return_summary.loc[ticker, 'Daily Volatility (Std Dev)'] + 0.0001,\n",
        "             risk_return_summary.loc[ticker, 'Mean Daily Return'],\n",
        "             ticker, fontsize=9)\n",
        "plt.xlabel('Daily Volatility (Std Dev)')\n",
        "plt.ylabel('Mean Daily Return')\n",
        "plt.title('Risk-Return Scatter Plot')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dA4ltzABw_ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cov_matrix = returns.cov()\n",
        "corr_matrix = returns.corr()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cov_matrix, cmap='Blues', center=0)\n",
        "plt.title('Covariance Matrix')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vtB0ieGoxI_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_returns(returns, n_lags):\n",
        "    X_all = []\n",
        "    y_all_dict = {ticker: [] for ticker in returns.columns}\n",
        "\n",
        "    for i in range(n_lags, len(returns) - 1):\n",
        "        lagged = returns.iloc[i - n_lags:i].values.flatten()\n",
        "        X_all.append(lagged)\n",
        "        for ticker in returns.columns:\n",
        "            y_all_dict[ticker].append(returns.iloc[i + 1][ticker])\n",
        "\n",
        "    X = np.array(X_all)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    latest_input = returns.iloc[-n_lags:].values.flatten().reshape(1, -1)\n",
        "    latest_input_scaled = scaler.transform(latest_input)\n",
        "\n",
        "    predicted_returns = []\n",
        "    for ticker in returns.columns:\n",
        "        y = np.array(y_all_dict[ticker])\n",
        "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "        model.fit(X_scaled[:len(y)], y)\n",
        "        pred = model.predict(latest_input_scaled)[0]\n",
        "        predicted_returns.append(pred)\n",
        "    return np.array(predicted_returns)"
      ],
      "metadata": {
        "id": "azqVAd77xN5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_portfolio(mu, Sigma, rf, tickers, max_variance=0.0002):\n",
        "    n = len(mu)\n",
        "\n",
        "    # MVO (Maximize Return under risk constraint)\n",
        "    mu_hist = np.mean(mu)\n",
        "    shrinkage_factor = 0.5\n",
        "    mu_shrunk = shrinkage_factor * mu + (1 - shrinkage_factor) * mu_hist\n",
        "    w_mvo = cp.Variable(n)\n",
        "    portfolio_return = mu_shrunk @ w_mvo\n",
        "    portfolio_variance = cp.quad_form(w_mvo, Sigma)\n",
        "    max_weight = 0.3\n",
        "    constraints = [\n",
        "    cp.sum(w_mvo) == 1,      # weights sum to 1\n",
        "    w_mvo >= 0,              # no short selling\n",
        "    w_mvo <= max_weight,     # max weight constraint\n",
        "    portfolio_variance <= max_variance  # risk constraint\n",
        "]\n",
        "    prob = cp.Problem(cp.Maximize(portfolio_return), constraints)\n",
        "    prob.solve()\n",
        "    weights_mvo = w_mvo.value\n",
        "\n",
        "    # Max Sharpe\n",
        "    def neg_sharpe(w):\n",
        "        ret = np.dot(w, mu)\n",
        "        vol = np.sqrt(np.dot(w.T, np.dot(Sigma, w)))\n",
        "        return -(ret - rf) / vol\n",
        "\n",
        "    bounds = [(0, 0.2)] * n\n",
        "    constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
        "    init_guess = np.repeat(1/n, n)\n",
        "\n",
        "    result = minimize(neg_sharpe, init_guess, method='SLSQP',\n",
        "                      bounds=bounds, constraints=constraints)\n",
        "    weights_sharpe = result.x\n",
        "\n",
        "    ret = np.dot(weights_sharpe, mu)\n",
        "    vol = np.sqrt(np.dot(weights_sharpe.T, np.dot(Sigma, weights_sharpe)))\n",
        "    sharpe = (ret - rf) / vol\n",
        "\n",
        "    return weights_mvo, weights_sharpe, ret, vol, sharpe"
      ],
      "metadata": {
        "id": "7jftDRcAxPtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_weight_portfolio(mu, Sigma, rf):\n",
        "    n = len(mu)\n",
        "    w_eq = np.repeat(1/n, n)\n",
        "    ret = np.dot(w_eq, mu)\n",
        "    vol = np.sqrt(np.dot(w_eq.T, np.dot(Sigma, w_eq)))\n",
        "    sharpe = (ret - rf) / vol\n",
        "    return w_eq, ret, vol, sharpe"
      ],
      "metadata": {
        "id": "hre3uWt7xTPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _nearest_psd(A, eps=1e-10):\n",
        "    B = 0.5*(A + A.T)\n",
        "    w, V = np.linalg.eigh(B)\n",
        "    w_clipped = np.clip(w, eps, None)\n",
        "    return (V * w_clipped) @ V.T\n",
        "\n",
        "def market_implied_delta(returns, rf, market_weights):\n",
        "    mu_mkt = returns.mean().values @ market_weights\n",
        "    var_mkt = market_weights.T @ returns.cov().values @ market_weights\n",
        "    delta = (mu_mkt - rf) / max(var_mkt, 1e-12)   # avoid divide-by-zero\n",
        "    return float(max(delta, 0.0))\n",
        "\n",
        "def black_litterman(mu_view, Sigma, rf, tickers, returns, tau=0.2, omega_scalar=0.1):\n",
        "    n = len(mu_view)\n",
        "    caps = []\n",
        "    for tk in tickers:\n",
        "        try:\n",
        "            info = yf.Ticker(tk).info\n",
        "            caps.append(info.get(\"marketCap\", 0))\n",
        "        except Exception:\n",
        "            caps.append(0)\n",
        "    caps = np.array(caps, dtype=float)\n",
        "    if np.nansum(caps) <= 0:\n",
        "        market_weights = np.full(n, 1.0/n)\n",
        "    else:\n",
        "        market_weights = caps/np.nansum(caps)\n",
        "    mu_view = np.asarray(mu_view, dtype=float).reshape(-1)\n",
        "    Sigma = np.asarray(Sigma, dtype=float)\n",
        "    Sigma_psd = _nearest_psd(Sigma)\n",
        "\n",
        "    # --- market-implied equilibrium returns ---\n",
        "    delta = market_implied_delta(returns, rf, market_weights)  # >= 0 by clamp\n",
        "    Pi = delta * (Sigma_psd @ market_weights)\n",
        "\n",
        "    # --- Blackâ€“Litterman posterior mean ---\n",
        "    P = np.eye(n)\n",
        "    Omega = np.eye(n) * omega_scalar\n",
        "    A = np.linalg.inv(tau * Sigma_psd)\n",
        "    post_prec = A + P.T @ np.linalg.inv(Omega) @ P\n",
        "    post_mean = np.linalg.inv(post_prec) @ (A @ Pi + P.T @ np.linalg.inv(Omega) @ mu_view)\n",
        "\n",
        "    # --- mean-variance portfolio\n",
        "    w = cp.Variable(n)\n",
        "    ret = post_mean @ w\n",
        "    risk = cp.quad_form(w, Sigma_psd)\n",
        "    constraints = [cp.sum(w) == 1, w >= 0]\n",
        "    prob = cp.Problem(cp.Maximize(ret - delta * risk), constraints)\n",
        "    prob.solve(solver=cp.SCS, verbose=False)\n",
        "\n",
        "    if prob.status not in (\"optimal\", \"optimal_inaccurate\"):\n",
        "        target = float(np.dot(post_mean, market_weights))  # market-implied return\n",
        "        w2 = cp.Variable(n)\n",
        "        risk2 = cp.quad_form(w2, Sigma_psd)\n",
        "        cons2 = [cp.sum(w2) == 1, w2 >= 0, post_mean @ w2 >= target]\n",
        "        prob2 = cp.Problem(cp.Minimize(risk2), cons2)\n",
        "        prob2.solve(solver=cp.SCS, verbose=False)\n",
        "        if prob2.status not in (\"optimal\", \"optimal_inaccurate\"):\n",
        "            raise ValueError(f\"BL optimization failed: {prob.status}, fallback: {prob2.status}\")\n",
        "        return post_mean, w2.value\n",
        "\n",
        "    return post_mean, w.value"
      ],
      "metadata": {
        "id": "YC4lJIJ_xVvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure returns is a DataFrame\n",
        "if isinstance(returns, pd.Series):\n",
        "    returns = returns.to_frame()\n",
        "\n",
        "# Replace inf values with NaN\n",
        "returns.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Drop rows (or columns) with NaN values\n",
        "returns.dropna(inplace=True)  # or use returns.fillna(0) based on your case\n",
        "\n",
        "# Now apply Z-score normalization\n",
        "zscore_returns = pd.DataFrame(StandardScaler().fit_transform(returns),\n",
        "                              columns=returns.columns, index=returns.index)\n"
      ],
      "metadata": {
        "id": "UoXKHS3ZxaiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming NIFTY index as market return\n",
        "nifty_index_data = yf.download(\"^NSEI\", start=\"2020-01-01\", end=\"2025-01-01\", auto_adjust=False)\n",
        "nifty_index = nifty_index_data['Adj Close']\n",
        "market_returns = nifty_index.pct_change().dropna()\n",
        "market_returns = market_returns.reindex(returns.index).fillna(method='ffill')\n",
        "\n",
        "capm_results = {}\n",
        "for stock in returns.columns:\n",
        "    X = sm.add_constant(market_returns)\n",
        "    y = returns[stock]\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    capm_results[stock] = model.params\n",
        "\n",
        "capm_df = pd.DataFrame(capm_results).T\n",
        "capm_df.columns = ['Alpha', 'Beta']\n",
        "print(capm_df)"
      ],
      "metadata": {
        "id": "uk_cC88bxbFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with NaN values from the returns DataFrame\n",
        "returns.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "laQfYcQHxfMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Black-Litterman Model\n",
        "# Define risk-free rate (replace with appropriate value)\n",
        "rf = 0.0  # Assuming a risk-free rate of 0 for daily returns\n",
        "\n",
        "# Calculate mean returns and covariance matrix from the returns data\n",
        "mu = returns.mean()\n",
        "Sigma = returns.cov()\n",
        "\n",
        "# Using historical mean as a placeholder for mu_view\n",
        "mu_view = mu.values\n",
        "\n",
        "import cvxpy as cp\n",
        "\n",
        "try:\n",
        "    mu_bl, weights_bl = black_litterman(mu_view, Sigma, rf, nifty50_tickers, returns)\n",
        "\n",
        "    # Create a DataFrame for Black-Litterman Weights and Posterior Returns\n",
        "    df_bl = pd.DataFrame({\n",
        "        'Ticker': nifty50_tickers,\n",
        "        'Weight (Black-Litterman)': weights_bl,\n",
        "        'Posterior Return (BL)': mu_bl\n",
        "    })\n",
        "\n",
        "    print(\"\\nBlack-Litterman Weights and Posterior Expected Returns:\")\n",
        "    print(df_bl.to_string(index=False))\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Black-Litterman optimization failed: {e}\")\n",
        "    # Handle the case where BL optimization fails"
      ],
      "metadata": {
        "id": "FrwL6Kxbxj-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Equal-Weight Portfolio Model\n",
        "# Calculate mean returns and covariance matrix from the returns data\n",
        "mu = returns.mean()\n",
        "Sigma = returns.cov()\n",
        "\n",
        "# Define risk-free rate (replace with appropriate value)\n",
        "rf = 0.0 # Assuming a risk-free rate of 0 for daily returns\n",
        "\n",
        "# Calculate Equal-Weight Portfolio\n",
        "w_eq, ret_eq, vol_eq, sharpe_eq = equal_weight_portfolio(mu, Sigma, rf)\n",
        "\n",
        "# Create a DataFrame for Equal-Weight Portfolio Weights\n",
        "df_eq = pd.DataFrame({\n",
        "    'Ticker': nifty50_tickers,\n",
        "    'Weight (Equal-Weight)': w_eq\n",
        "})\n",
        "\n",
        "# Create a DataFrame for Equal-Weight Portfolio Summary\n",
        "df_eq_summary = pd.DataFrame({\n",
        "    'Metric': ['Return', 'Volatility', 'Sharpe Ratio'],\n",
        "    'Value': [ret_eq, vol_eq, sharpe_eq]\n",
        "})\n",
        "\n",
        "print(\"\\nEqual-Weight Portfolio Weights:\")\n",
        "print(df_eq.to_string(index=False))\n",
        "\n",
        "print(\"\\nEqual-Weight Portfolio Summary:\")\n",
        "print(df_eq_summary.to_string(index=False))"
      ],
      "metadata": {
        "id": "e1dpLjE1xoDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare CAPM, Black-Litterman, and Equal-Weight Models\n",
        "\n",
        "# Collect relevant metrics\n",
        "comparison_metrics = {}\n",
        "\n",
        "# Get CAPM Alpha and Beta (average across stocks) if available\n",
        "if 'capm_df' in locals() and not capm_df.empty:\n",
        "    comparison_metrics['CAPM'] = {\n",
        "        'Average Alpha': capm_df['Alpha'].mean(),\n",
        "        'Average Beta': capm_df['Beta'].mean(),\n",
        "        'Sharpe Ratio': np.nan # CAPM itself doesn't provide a portfolio Sharpe Ratio directly\n",
        "    }\n",
        "\n",
        "# Get Black-Litterman Sharpe Ratio if available\n",
        "if 'weights_bl' in locals() and 'mu_bl' in locals() and 'Sigma' in locals():\n",
        "    try:\n",
        "        # Calculate Sharpe Ratio for Black-Litterman portfolio\n",
        "        ret_bl = np.dot(weights_bl, mu_bl) * 252\n",
        "        Sigma_np = Sigma.values if isinstance(Sigma, pd.DataFrame) else Sigma\n",
        "        vol_bl = np.sqrt(np.dot(weights_bl.T, np.dot(Sigma_np, weights_bl))) * np.sqrt(252)\n",
        "        sharpe_bl = (ret_bl - rf) / (vol_bl + 1e-9) # Add epsilon to avoid division by zero\n",
        "        if 'Black-Litterman' in comparison_metrics:\n",
        "            comparison_metrics['Black-Litterman']['Sharpe Ratio'] = sharpe_bl\n",
        "        else:\n",
        "            comparison_metrics['Black-Litterman'] = {'Average Alpha': np.nan, 'Average Beta': np.nan, 'Sharpe Ratio': sharpe_bl}\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating Sharpe Ratio for Black-Litterman for comparison: {e}\")\n",
        "\n",
        "\n",
        "# Get Equal-Weight Sharpe Ratio if available\n",
        "if 'df_eq_summary' in locals() and not df_eq_summary.empty:\n",
        "    eq_sharpe = df_eq_summary[df_eq_summary['Metric'] == 'Sharpe Ratio']['Value'].iloc[0]\n",
        "    if 'Equal-Weight' in comparison_metrics:\n",
        "        comparison_metrics['Equal-Weight']['Sharpe Ratio'] = eq_sharpe\n",
        "    else:\n",
        "         comparison_metrics['Equal-Weight'] = {'Average Alpha': np.nan, 'Average Beta': np.nan, 'Sharpe Ratio': eq_sharpe}\n",
        "\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "# Create a DataFrame for comparison\n",
        "comparison_df = pd.DataFrame.from_dict(comparison_metrics, orient='index')\n",
        "\n",
        "# Clean up NaN values for better display\n",
        "comparison_df = comparison_df.fillna('')   # replace NaN with blank\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(comparison_df)\n"
      ],
      "metadata": {
        "id": "1CK3mf-6xwQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'comparison_metrics' dictionary is available from the previous cell's execution\n",
        "if 'comparison_metrics' in locals():\n",
        "    # Extract Sharpe Ratios for the three portfolio models\n",
        "    sharpe_ratios = {\n",
        "        'CAPM': comparison_metrics.get('CAPM', {}).get('Sharpe Ratio'),\n",
        "        'Black-Litterman': comparison_metrics.get('Black-Litterman', {}).get('Sharpe Ratio'),\n",
        "        'Equal-Weight': comparison_metrics.get('Equal-Weight', {}).get('Sharpe Ratio')\n",
        "    }\n",
        "\n",
        "    # Remove models with no calculated Sharpe Ratio (NaN)\n",
        "    sharpe_ratios_cleaned = {k: v for k, v in sharpe_ratios.items() if pd.notna(v)}\n",
        "\n",
        "    if sharpe_ratios_cleaned:\n",
        "        models = list(sharpe_ratios_cleaned.keys())\n",
        "        sharpes = list(sharpe_ratios_cleaned.values())\n",
        "\n",
        "        # Create a bar plot to compare Sharpe Ratios\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        bars = plt.bar(models, sharpes, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "        plt.ylabel('Sharpe Ratio')\n",
        "        plt.title('Sharpe Ratio Comparison of Portfolio Models')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Add the values on top of the bars\n",
        "        for bar in bars:\n",
        "            yval = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center') # Adjust va and ha for positioning\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid Sharpe Ratios available for plotting the comparison.\")\n",
        "\n",
        "else:\n",
        "    print(\"Model comparison metrics not available. Please run the cell to calculate comparison_metrics.\")"
      ],
      "metadata": {
        "id": "Bt7dYRYgxzmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a time series graph of daily returns for selected stocks\n",
        "\n",
        "# Select a few stocks to visualize\n",
        "selected_stocks = ['RELIANCE.NS', 'TCS.NS', 'HDFCBANK.NS', 'INFY.NS']\n",
        "\n",
        "if all(stock in returns.columns for stock in selected_stocks):\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    for stock in selected_stocks:\n",
        "        returns[stock].plot(label=stock, alpha=0.7)\n",
        "\n",
        "    plt.title('Daily Returns Time Series for Selected Stocks')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Daily Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"One or more selected stocks not found in the returns data.\")"
      ],
      "metadata": {
        "id": "uu0FpYKTx72Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate annual Sharpe Ratio for each year from 2021 to 2024\n",
        "\n",
        "annual_sharpe_ratios = {}\n",
        "\n",
        "for year in range(2021, 2025):\n",
        "    year_str = str(year)\n",
        "    returns_year = returns.loc[year_str]\n",
        "\n",
        "    if not returns_year.empty:\n",
        "        mean_returns_year = returns_year.mean()\n",
        "        cov_matrix_year = returns_year.cov()\n",
        "\n",
        "        # Assuming an equally-weighted portfolio for simplicity for annual Sharpe calculation\n",
        "        # You could also optimize for each year, but that's more complex and might overfit\n",
        "        n_assets = returns_year.shape[1]\n",
        "        equal_weights = np.repeat(1.0/n_assets, n_assets)\n",
        "\n",
        "        # Calculate annual performance metrics\n",
        "        # Need to annualize daily metrics: multiply mean by 252, std dev by sqrt(252)\n",
        "        annual_mean_return = mean_returns_year.mean() * 252\n",
        "        annual_volatility = returns_year.std().mean() * np.sqrt(252) # Using mean of daily std dev for simplicity\n",
        "\n",
        "        # Calculate Sharpe Ratio (assuming risk-free rate is 0)\n",
        "        annual_sharpe = annual_mean_return / (annual_volatility + 1e-9) # Add epsilon to avoid division by zero\n",
        "\n",
        "        annual_sharpe_ratios[year_str] = annual_sharpe\n",
        "    else:\n",
        "        print(f\"No returns data for year {year}\")\n",
        "\n",
        "\n",
        "# Visualize annual Sharpe Ratios\n",
        "if annual_sharpe_ratios:\n",
        "    years = list(annual_sharpe_ratios.keys())\n",
        "    sharpes = list(annual_sharpe_ratios.values())\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    bars = plt.bar(years, sharpes, color='skyblue')\n",
        "    plt.ylabel('Sharpe Ratio')\n",
        "    plt.title('Annual Sharpe Ratio (2021-2024)')\n",
        "    plt.grid(axis='y')\n",
        "\n",
        "    # Add the values on top of the bars\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center')\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No annual Sharpe Ratios calculated for visualization.\")"
      ],
      "metadata": {
        "id": "YYEOzVeNx_PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Cumulative Return for CAPM-based portfolio from 2021 to 2025\n",
        "\n",
        "# Ensure CAPM results, market returns, and risk-free rate are available\n",
        "if 'capm_df' in locals() and 'market_returns' in locals() and 'rf' in locals():\n",
        "    # Select the data for the specified period\n",
        "    returns_2021_2025 = returns.loc['2021':'2025']\n",
        "    market_returns_2021_2025 = market_returns.loc['2021':'2025']\n",
        "\n",
        "    if not returns_2021_2025.empty and not market_returns_2021_2025.empty:\n",
        "        # Calculate expected returns for each stock using CAPM for the period\n",
        "        # E(Ri) = Rf + Beta_i * (E(Rm) - Rf)\n",
        "        # We can use the average market return over the period as an estimate for E(Rm)\n",
        "        average_market_return_2021_2025 = market_returns_2021_2025.mean()\n",
        "\n",
        "        # Ensure capm_df is aligned with the stocks in returns_2021_2025\n",
        "        capm_df_aligned = capm_df.reindex(returns_2021_2025.columns)\n",
        "\n",
        "        # Calculate expected returns based on CAPM\n",
        "        expected_returns_capm = rf + capm_df_aligned['Beta'] * (average_market_return_2021_2025 - rf)\n",
        "\n",
        "        # For simplicity, let's assume an equally-weighted portfolio based on CAPM expected returns\n",
        "        # A more sophisticated approach would be to optimize the portfolio based on these expected returns\n",
        "        n_assets = returns_2021_2025.shape[1]\n",
        "        equal_weights = np.repeat(1.0 / n_assets, n_assets)\n",
        "\n",
        "        # Calculate the daily portfolio return using the actual returns and equal weights\n",
        "        capm_portfolio_daily_return = returns_2021_2025.dot(equal_weights)\n",
        "\n",
        "        # Calculate the cumulative return\n",
        "        capm_cumulative_return = (1 + capm_portfolio_daily_return).cumprod()\n",
        "\n",
        "        print(\"\\nCAPM-based Equally-Weighted Portfolio Cumulative Return from 2021 to 2025:\")\n",
        "        print(capm_cumulative_return.iloc[-1])  # Print the final cumulative return\n",
        "\n",
        "        # Optional: Plot the cumulative return\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        capm_cumulative_return.plot()\n",
        "        plt.title('CAPM-based Equally-Weighted Portfolio Cumulative Return (2021-2025)')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Cumulative Return')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"No returns data available for the period 2021-2025.\")\n",
        "\n",
        "else:\n",
        "    print(\"CAPM results, market returns, or risk-free rate not available.\")"
      ],
      "metadata": {
        "id": "yTmsyovPyDbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Cumulative Return from 2021 to 2025\n",
        "\n",
        "# Select the returns data from 2021 to 2025\n",
        "returns_2021_2025 = returns.loc['2021':'2025']\n",
        "\n",
        "if not returns_2021_2025.empty:\n",
        "    # Calculate the daily average return of an equally-weighted portfolio\n",
        "    equal_weight_daily_return = returns_2021_2025.mean(axis=1)\n",
        "\n",
        "    # Calculate the cumulative return\n",
        "    cumulative_return = (1 + equal_weight_daily_return).cumprod()\n",
        "\n",
        "    print(\"Cumulative Return from 2021 to 2025:\")\n",
        "    print(cumulative_return.iloc[-1]) # Print the final cumulative return\n",
        "\n",
        "    # Optional: Plot the cumulative return\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    cumulative_return.plot()\n",
        "    plt.title('Equally-Weighted Portfolio Cumulative Return (2021-2025)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Cumulative Return')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No returns data available for the period 2021-2025.\")"
      ],
      "metadata": {
        "id": "N4UeJP4lyGcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Cumulative Return for Black-Litterman portfolio from 2021 to 2025\n",
        "\n",
        "# Ensure Black-Litterman weights and returns data are available\n",
        "if 'weights_bl' in locals() and 'returns' in locals():\n",
        "    # Select the returns data for the specified period\n",
        "    returns_2021_2025 = returns.loc['2021':'2025']\n",
        "\n",
        "    if not returns_2021_2025.empty:\n",
        "        # Ensure the order of weights matches the columns in returns_2021_2025\n",
        "        # Assuming nifty50_tickers list was used to generate the returns columns\n",
        "        # and weights_bl corresponds to this order.\n",
        "        # If not, you might need to reindex weights_bl based on returns_2021_2025.columns\n",
        "        try:\n",
        "            # Calculate the daily portfolio return using the actual returns and BL weights\n",
        "            bl_portfolio_daily_return = returns_2021_2025.dot(weights_bl)\n",
        "\n",
        "            # Calculate the cumulative return\n",
        "            bl_cumulative_return = (1 + bl_portfolio_daily_return).cumprod()\n",
        "\n",
        "            print(\"\\nBlack-Litterman Portfolio Cumulative Return from 2021 to 2025:\")\n",
        "            print(bl_cumulative_return.iloc[-1])  # Print the final cumulative return\n",
        "\n",
        "            # Optional: Plot the cumulative return\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            bl_cumulative_return.plot()\n",
        "            plt.title('Black-Litterman Portfolio Cumulative Return (2021-2025)')\n",
        "            plt.xlabel('Date')\n",
        "            plt.ylabel('Cumulative Return')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        except ValueError as e:\n",
        "             print(f\"Error calculating Black-Litterman portfolio return. Ensure weights align with returns data: {e}\")\n",
        "    else:\n",
        "        print(\"No returns data available for the period 2021-2025.\")\n",
        "\n",
        "else:\n",
        "    print(\"Black-Litterman weights or returns data not available.\")"
      ],
      "metadata": {
        "id": "7AfGHTpByKnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot all three cumulative returns together\n",
        "\n",
        "# Ensure cumulative return dataframes are available\n",
        "if 'capm_cumulative_return' in locals() and 'bl_cumulative_return' in locals() and 'cumulative_return' in locals():\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot each cumulative return series\n",
        "    capm_cumulative_return.plot(label='CAPM-based Portfolio', alpha=0.7)\n",
        "    bl_cumulative_return.plot(label='Black-Litterman Portfolio', alpha=0.7)\n",
        "    cumulative_return.plot(label='Equal-Weight Portfolio', alpha=0.7)\n",
        "\n",
        "\n",
        "    plt.title('Cumulative Return Comparison (2021-2025)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Cumulative Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cumulative return data for all models not available for plotting.\")"
      ],
      "metadata": {
        "id": "Cdd87jVgyN0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cvxpy as cp\n",
        "\n",
        "# Assuming 'returns', 'mean_returns', and 'Sigma' are available from previous cells\n",
        "\n",
        "if 'mean_returns' in locals() and 'Sigma' in locals() and not mean_returns.empty and not Sigma.empty:\n",
        "    n_assets = len(mean_returns)\n",
        "    returns_np = mean_returns.values\n",
        "    Sigma_np = Sigma.values\n",
        "\n",
        "    # Ensure Sigma is positive semi-definite\n",
        "    def _nearest_psd(A, eps=1e-10):\n",
        "        B = (A + A.T) / 2\n",
        "        _, s, V = np.linalg.svd(B)\n",
        "        s = np.maximum(s, eps)\n",
        "        return V @ np.diag(s) @ V.T\n",
        "\n",
        "    Sigma_psd = _nearest_psd(Sigma_np)\n",
        "\n",
        "    # Define the number of points for the efficient frontier\n",
        "    n_portfolios = 100\n",
        "    # Use daily mean returns for the target returns range\n",
        "    target_returns_daily = np.linspace(mean_returns.min(), mean_returns.max(), n_portfolios)\n",
        "\n",
        "    portfolio_risks_daily = []\n",
        "    portfolio_returns_daily = []\n",
        "\n",
        "    for target_return_daily in target_returns_daily:\n",
        "        weights = cp.Variable(n_assets)\n",
        "        risk = cp.quad_form(weights, Sigma_psd)\n",
        "        constraints = [\n",
        "            cp.sum(weights) == 1,\n",
        "            weights >= 0,\n",
        "            returns_np @ weights >= target_return_daily # Use daily target return\n",
        "        ]\n",
        "        prob = cp.Problem(cp.Minimize(risk), constraints)\n",
        "        try:\n",
        "            prob.solve(solver=cp.SCS, verbose=False)\n",
        "            if prob.status in (\"optimal\", \"optimal_inaccurate\"):\n",
        "                portfolio_risks_daily.append(np.sqrt(prob.value)) # Daily risk (standard deviation)\n",
        "                portfolio_returns_daily.append(target_return_daily)  # Daily return\n",
        "        except cp.error.SolverError:\n",
        "            # Handle cases where the solver fails for a given target return\n",
        "            print(f\"Solver failed for target return: {target_return_daily:.6f}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "    # Plot the efficient frontier (daily)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(portfolio_risks_daily, portfolio_returns_daily, marker='o', s=10)\n",
        "    plt.xlabel('Daily Volatility (Risk)')\n",
        "    plt.ylabel('Daily Return')\n",
        "    plt.title('Efficient Frontier (Daily)')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Mean returns or covariance matrix not available for plotting the efficient frontier.\")"
      ],
      "metadata": {
        "id": "amhF2ROSyRdt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}